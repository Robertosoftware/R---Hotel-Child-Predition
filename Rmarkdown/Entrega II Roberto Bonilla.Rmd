---
title: "Entrega II Individual"
description: |
  Entrega SEMMA
author:
  - name: Roberto Bonilla Ibarra
    url: 
    affiliation: Universidad Complutense de Madrid
    affiliation_url: 
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        colorlinks: true
        code_folding: false
        toc: true            
        toc_depth: 3     
---

```{r setup, include = FALSE}
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 8, fig.asp = 1, out.width = "100%",
                      message = FALSE, warning = FALSE,
                      echo = TRUE, res = 400)
```

# Objetivo {#objetivo}

El objetivo de esta entrega es poder predecir si los individuos recibirán un sueldo mayor o menor o igual a 50 mil dólares mensuales basado en sus características socioeconómicas, usando tres diferentes algoritmos de clasificación (knn, árboles y random forest).

## Paquetes necesarios

Necesitaremos los siguientes paquetes

* **Análisis exploratorio numérico**: paquete `{skimr}`
* **Depuración y preprocesamiento**: paquete `{tidyverse}`
* **Modelización**: paquete `{tidymodels}` para modelos
* **Detección de outliers**: paquete `{outliers}`


```{r paquetes}
# Borramos
rm(list = ls())

# Paquetes
library(skimr) # resumen numérico
library(tidymodels) # depuración datos
library(tidyverse) # modelos
library(outliers) # outliers
library(timeDate) # fechas
library(ggthemes) # tema para graficar
```

# Datos {#datos}

Vamos ir a un **ejemplo real**, haciendo uso de un **dataset de adultos con distintas variables socioeconómicas.**

```{r}
adultos <- read_csv(file = "./datos/adults.csv")
```

Los datos forman parte de un **censo poblacional relacionado a la percepción económica** elaborado por US Census Bureau con 32,561 registros.

📚 **Detalle de variables**: <http://www.cs.toronto.edu/~delve/data/adult/adultDetail.html1>



## Análisis exploratorio inicial (numérico)

Antes de tomar ninguna decisión con los datos lo primero que deberíamos hacer es **echar un vistazo numérico** a cómo se comportan las variables. Dado que vamos a clasificar, lo primero que deberíamos observar es como se distribuyen los niveles de nuestra variable objetivo.

### Variables

Lo primero es conocer las variables

```{r}
glimpse(adultos)
```

* `age`: Edad de los individuos
* `workclass`: Tipo de trabajo de cada individuo
* `fnlwgt`: Número de personas con la misma edad y raza.
* `education,education_num`: Nivel de educación máximo
* `marital_status`: Situación sentimental del individuo.
* `occupation`: A lo que se dedica el individuo
* `relationship`: Cuál es la relación que tiene con su familia.
* `race`: Raza del individuo
* `sex`: Sexo del individuo
* `capital_gain`: Capital percibido
* `capital_loss`: Capital perdido
* `hours_per_week`: Horas de trabajo semanales
* `native_country`: País de origen
* `over_50k`: Si el individuo percibe un ingreso mayor a 50 mil dólares anuales o menor o igual a este.


Además con la función `skim()` del paquete `{skimr}` podemos **extraer algunas estadísticas básicas** de nuestros datos.

```{r skim}
# Resumen numérico
adultos |> skim()
```

Veamos los tipos de dato de las variables:

```{r}
sapply(adultos, class)
```

Podemos ver que tenemos que cambiar el tipo de dato de la variable capital loss y capital gain, veamos su distribución,

**Capital Gain**
```{r}
adultos |> 
  count(capital_gain) |>
  mutate(porc = 100*n/sum(n))

```

**Capital Loss**

```{r}
adultos |> 
  count(capital_loss) |>
  mutate(porc = 100*n/sum(n))
```

Hemos visto que son variables continuas, por la tanto las convertiremos en numéricas:

```{r}
adultos$capital_gain <- as.numeric(str_replace_all(adultos$capital_gain,',',''))
adultos$capital_loss <- as.numeric(str_replace_all(adultos$capital_loss,',',''))
```


### Balance la variable objetivo

El objetivo será **predecir si un adulto está destinado a recibir un ingreso mayor a 50 mil dólares anuales o menor o igual a este**, por lo que la variable `over_50k` será nuestra variable objetivo. Primer paso: conocer cómo se **distribuyen los niveles de la objetivo** (es binaria)


```{r}
adultos |> 
  count(over_50k) |>
  mutate(porc = 100*n/sum(n))
```

Podemos ver que nuestra variable objetivo está desbalanceada hacia las personas con un ingreso menor o igual a 50 mil dólares.


# Análisis Exploratorio Visual

Antes de poder tomar decisiones en cómo transformar nuestros datos es importante poder hacer un análisis detallado visual de nuestros datos y conocer cómo interactúan con la variable objetivo:

Para esto usaremos el tema del economist del paquete ggplothemes

## Distribución visual variable objetivo

La Variable Objetivo se encuentre desbalanceda, teniendo un 75% de casos donde las personas ganan menos que 50 mil dólares al año.

```{r}
ggplot(adultos, aes(x = over_50k, fill = over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Variable objetivo",
       subtitle = "Distribución de ingresos",
       x = "Cantidad", y = "Frecuencia") +
  theme_economist()

```


## Distribución visual variables continuas

### Variable Edad

```{r}
ggplot(adultos, aes(x = age, fill = over_50k)) +
    geom_density(alpha = .3) +
    labs(title = "Variable Edad",
       subtitle = "Distribución de Edad con diferencia de Variable Objetivo",
       x = "Edad", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que las personas que ganan más de 50 mil dólares al año tienden a ser de mayor edad en comparación con los que hacen menos o igual a 50 mil dólares al año, también podemos ver que la mayoría están entre 30 y 55 años.

```{r}
ggplot(adultos, aes(x = over_50k, y = age, color=over_50k)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Variable Edad",
       subtitle = "Distribución de Edad por Tipo de Ingreso",
       x = "Over 50k", y = "Cantidad") +
  theme_economist()
```

### Variable Capital Gain

Ahora veamos como se comporta la variable de Capital Ganado

```{r}
ggplot(adultos, aes(x = capital_gain, fill = over_50k)) +
    geom_density(alpha = .6) + # Grado de transparencia para ver la gráfica traspuesta
  scale_y_continuous(limits = c(0, .0001)) + # Para poder visualizar mejor la gráfica
    labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado con diferencia de Variable Objetivo",
       x = "Capital Ganado", y = "Frecuencia") +
  theme_economist()
```


Podemos ver que las personas que perciben un ingreso mayor a 50K USD tienden a tener capital ganado en comparación con los que no perciben un ingreso mayor a 50k USD.

```{r}
ggplot(adultos |>  filter(capital_gain<20000), aes(x = capital_gain, fill = over_50k)) +
    geom_histogram(position = "identity", alpha = 0.4, bins=20) + # Posición identity para que se puedan trasponer los histogramas
     labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado con diferencia de Variable Objetivo",
       x = "Capital Gain", y = "Frecuencia") +
  theme_economist()
```


Podemos corroborar lo que mencionamos arriba, donde si bien la mayoría de las personas no ganan capital, cuando ellas documentan que perciben cierto capital tienen más probabilidad de ser del grupo de personas que perciben un ingreso mayor a 50 K USD.

```{r}
ggplot(adultos, aes(x = capital_gain, y = education, color=over_50k)) +
  geom_point() +
   labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado por Educación") +
  theme_economist()
```

Podemos ver que las personas que tienen un nivel académico bajo tienden a reportar un capital ganado bajo o nulo y a la vez este repercute en el ingreso anual que perciben en el año.

### Variable Capital Loss

Ahora analizaremos la variable de Capital Perdido

```{r}
ggplot(adultos, aes(x = over_50k, y = capital_loss, color=over_50k)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Variable Capital Perdido",
       subtitle = "Distribución del Capital Perdido",
       x = "Over 50k", y = "Cantidad") +
  theme_economist()
```

Podemos ver que los percentiles 25, 50 y 75 se encuentran en 0, señalando que la mayor parte de las personas no tienen capital perdido, sin embargo al ver la distribución podemos determinar que en el caso que una persona tenga perdida de capital, es muy probable que si ella percibe un ingreso menor a 50 mil USD sea baja y visceversa, determinando esto también por donde se posiciona la media de cada grupo. (Las personas conun ingreso más alto son menos en los casos de perdida, sin embargo, su media es más alta) 

```{r}

ggplot(adultos, aes(x = capital_loss, fill = over_50k)) +
    geom_density(alpha = .6) +
  scale_y_continuous(limits = c(0, .002)) +
    labs(title = "Variable Capital Perdido",
       subtitle = "Distribución de Capital Perdidoe con diferencia por Tipo de Ingreso",
       x = "Capital Perdido", y = "Frecuencia") +
  theme_economist()
```

Esta gráfica confirma la observación marcada anteriormente con respecto al capital perdido, lo importante a resaltas es que al ser está gráfica la densidad de la distribución se aprecia de mejor manera.

### Variable Hours_per_week

```{r}

ggplot(adultos, aes(x =  hours_per_week, fill = over_50k)) +
    geom_density(position = "identity", alpha = 0.4, bins=20) +
    labs(title = "Variable Horas a la Semana",
       subtitle = "Distribución de Horas a la Semana con diferencia de Variable Objetivo",
       x = "Horas a la Semana", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que las personas con un ingreso 

### Variable fnlwgt


```{r}

ggplot(adultos, aes(x = fnlwgt, fill = over_50k)) +
    geom_density(alpha = .3) +
    labs(title = "Variable fnlwgt",
       subtitle = "Distribución de Fnlwgt con diferencia de Variable Objetivo",
       x = "Fnlwgt", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que no existe ningún patrón entre las dos distribuciones.

## Distribución visual variables continuas

### Variable Clase de Trabajo

Analizaremos la variable **Clase de Trabajo** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = workclass, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Clases de Trabajo",
       subtitle = "Distribución de Clases de Trabajo",
       x = "Personas", y = "Clases de Trabajo") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

### Variable Estado Civil

Analizaremos la variable **Estado Civil** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = marital_status, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Estado Civil",
       subtitle = "Distribución del Estado Civil",
       x = "Personas", y = "Estado Civil") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver un comportamiento muy importante en la clase donde la gente se ha casado de manera civil, teniendo una gran cantidad y proporción mucho más favorable para los individuos que ganan más de 50 mil USD al año.

### Variable Ocupación

Analizaremos la variable *Ocupación** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = occupation, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Ocupación",
       subtitle = "Distribución de la variable Ocupación",
       x = "Personas", y = "Ocupación") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Como era de esperarse las clases que hacen una diferencia en su distribución son **Prof-Speciality** y **Exec-Managerial** por el tipo de ingreso que llevan estos dos grupos de personas.

### Variable Relación

Analizaremos la variable *Relación** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = relationship, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Relación",
       subtitle = "Distribución de la variable Relación",
       x = "Personas", y = "Relación") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Vemos aquí que los jedes de familia son en su mayoría los que perciben un ingreso mayor a 50 mil dólares anuales.

### Variable Raza

Analizaremos la variable *Raza** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = race, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Raza",
       subtitle = "Distribución de la variable Raza",
       x = "Personas", y = "Raza") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que los asiáticos y blancos son los más propensos a percibir un ingreso superior a 50 mil dólares anuales.

### Variable Sexo

Analizaremos la variable *Sexo** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = sex, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Sexo",
       subtitle = "Distribución de la variable Sexo",
       x = "Personas", y = "Sexo") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que al agruparse la población, el sexo masculino tiene una mayor posibilidad de percibir un ingreso superior a 50 mil dólares anuales.


### Variable País

Analizaremos la variable *País** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = native_country, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable País",
       subtitle = "Distribución de la variable País en %",
       x = "Personas", y = "País") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(round(count / sum(count),2) * 100)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que alrededor del 90% de las personas han nacido en Estados Unidos.

### Variable 

# Introducción Teórica

Explicaremos brevemente cada uno de los algoritmos que se usarán en esta práctica:


## KNN

Es uno de los modelos de Machine Learning más sencillos. 

Su lógica es asumir la similaridad entre un nuevo caso o dato y los otros puntos disponibles, colocando este nuevo punto en las categorías previamente definidas.

Algoritmo usado para regresión y clasificación.

Es un algoritmo de aprendizaje vago (lazy learner algorithm) ya que hace un almacenamiento de los datos de entrenamiento y espera hasta que reciba los datos de testing para poder empezar a clasificar, haciendo que tome menos tiempo en entrenamiento, pero más tiempo prediciendo.


El funcionamiento del algoritmo de KNN es el siguiente:

**Paso 1**: 
**Paso 2**:
**Paso 3**:
**Paso 4**:
**Paso 5**:
**Paso 6**:

```{r echo = FALSE, results = 'asis'}
image = "https://datascientest.com/es/wp-content/uploads/sites/7/2020/11/Illu-2-KNN-1024x492.jpg"
cat(paste0('<center><img src="', image,  '"></center>')) 
```

Fuente: <https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning>

## Árboles de decisión

Los arboles de decisión es un modelo de aprendizaje supervisado que sive para resolver problemas de clasificación y regresión, también conocido como `Classification And Regression Tree (CART) algorithm.`

Con base a la variable objetivo podemos definir los árboles de decisión en dos tipos:

1. Arboles de Decisión de Variables Categóricas
2. Arboles de Decisión de Variables Continuas

Para


## Random Forest





# Fase 1-2-3: muestreo-exploración-modificación

Ahora examinaremos los datos con el objetivo de poder tomar **decisiones que deberíamos adoptar**. Por ejemplo:

&nbsp;

* ¿Necesitamos **muestreo**? ¿De qué forma? ¿Podremos permitirnos crear un dataset de **validación**?

* ¿De qué **tipo** es cada variable? ¿Tenemos **problemas de codificación o rango**?

* ¿Cómo **afectan las predictoras** a los niveles de la variable objetivo?

* ¿Hay problemas de **dependencia** entre las variables?

* ¿Necesitamos **recategorizar** las variables? 

* ¿Tenemos **datos atípicos**?  ¿Tenemos **datos ausentes**? ¿Cómo imputarlos?

* ¿Necesitaremos hacer **One Hot Encoding** a nuestras variables categóricas?

&nbsp;

La **filosofía** será la siguiente: 

* modificaciones «estructurales» las hacemos fuera de la receta (modificando la base de datos)

* modificaciones más concretas para un algoritmo dentro de la receta (sin modificar la base de datos).

## Factores

Una de las primeras decisiones será dotar a las variables de su **tipología correcta**: debemos decidir si las variables de tipo texto son **variables cualitativas** (factores) o meros id's.

```{r}
adultos |>
  select(where(is.character)) |>
  glimpse()
```


Todas las variables de tipo texto representan **categorías de una cualitativa** así que las convertimos todas ellas a factor (modificación estructural --> fuera de la receta)

```{r}

adultos <- 
  adultos |>
  mutate(across(where(is.character),  ~str_replace_all(., ",", "")), across(where(is.character), as_factor))
adultos |> select(where(is.factor))
```

### Ordinales

Podemos ver que de todas nuestras variables la educación puede tener un orden en sus variables.

Primero veamos su distribución:

```{r}
adultos |>
  count(education) |> 
  mutate(porc = 100*n/sum(n))
```

Vemos que son 16 niveles de variable que reduciremos a 5 niveles.

Pero primero les quitaremos la coma al final


```{r}

adultos <- adultos |> 
 mutate(
    education_transformed = as.factor(case_when(
      education == "Doctorate"  ~ as.character("Grado 5"),
      education == "Prof-school" | education == "Masters"  ~ as.character("Grado 4"),
      education == "Some-college" | education == "Bachelors"  ~ as.character("Grado 3"),
      education == "Assoc-acdm" | education == "Assoc-voc"  ~ as.character("Grado 2"),
      TRUE ~ as.character("Grado 1")
    )))

knitr::kable(adultos |> select(education, education_transformed) |>  unique())
```


Tras ello convertiremos `education_transformed` a cualitativa pero ordinal.

```{r}
adultos <-
  adultos |>
  mutate(education_transformed = factor(education_transformed, levels = c("Grado 1", "Grado 2", "Grado 3", "Grado 4", "Grado 5"),
                       ordered = TRUE))
knitr::kable(adultos |> select(education_transformed) |> arrange(desc(education_transformed)) |> unique() )
```


## Variables cuali

Una vez convertidas en cualitativas analicemos cada una de ellas. La idea básica es la siguiente: ver que peso suponen cada nivel en las variables, y además, ver como **afectan los niveles a la variable objetivo**.

### Variable workclass

Ahora procederemos a ver cómo se distribuye la variable **clase de trabajo**:

```{r}
adultos |>
  count(workclass, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(workclass) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Self-emp-inc` y `Self-emp-not-inc` en un nuevo grupo llamado `Self-Emp` , así como `Local-Gov` y `State-Gov` en `Other-Gov`. También renombraremos el tipo **?** a `Desconocido`.

```{r}

adultos <- adultos |> 
 mutate(
    workclass_transformed = as.factor(case_when(
      workclass == "?"  ~ as.character("Desconocido"),
      workclass == "Local-gov" | workclass == "State-gov"  ~ as.character("Other-Gov"),
      workclass == "Self-emp-inc" | workclass == "Self-emp-not-inc"  ~ as.character("Self-Emp"),
      TRUE ~ as.character(workclass)
    )))

knitr::kable(adultos |> select(workclass, workclass_transformed) |>  unique())
```

### Variable Estado Civil

Ahora procederemos a ver cómo se distribuye la variable **Estado Civil**:

```{r}
adultos |>
  count(marital_status, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(marital_status) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Married-civ-spouse` y `Married-AF-spouse` en un nuevo grupo llamado `Married` , así como `Divorced`, `Separated`, `Widowed` y `Married-spouse-absent` en `Complicated`.

```{r}

adultos <- adultos |> 
 mutate(
    marital_status_transformed = as.factor(case_when(
      marital_status == "Married-civ-spouse" | marital_status == "Married-AF-spouse"  ~ as.character("Married"),
      marital_status == "Never-married"  ~ as.character("Never-married"),
      TRUE ~ as.character("Complicated")
    )))

knitr::kable(adultos |> select(marital_status, marital_status_transformed) |>  unique())
```



### Variable Ocupación

Ahora procederemos a ver cómo se distribuye la variable **Ocupación**:

```{r}
adultos |>
  count(occupation, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(occupation) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Prof-specialty` y `Exec-managerial` en un nuevo grupo llamado `High-Income` , así como `Protective-serv`, `Tech-support` y `Sales` en `Medium-Income` y el resto en `Low-Income`.

```{r}

adultos <- adultos |> 
 mutate(
    occupation_transformed = as.factor(case_when(
      occupation == "Prof-specialty" | occupation == "Exec-managerial"  ~ as.character("High-Income"),
      occupation == "Protective-serv" | occupation == "Tech-support" | occupation == "Sales"   ~ as.character("Medium-Income"),
      TRUE ~ as.character("Low-Income")
    )))

knitr::kable(adultos |> select(occupation, occupation_transformed) |>  unique())
```

### Variable Relación

Ahora procederemos a ver cómo se distribuye la variable **Relación**:

```{r}
adultos |>
  count(relationship, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(relationship) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que pueden hacer sentido como  `Wife` y `Husband` en un nuevo grupo llamado `Boss` , así como el resto en un grupo llamado `Otros`. Esto lo haremos directamente en la receta para dejar al algorimto KNN con esta transformación y el resto de los algoritmos sin esta transformación.


### Variable Raza

Ahora procederemos a ver cómo se distribuye la variable **Raza**:

```{r}
adultos |>
  count(race, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(race) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso solo hace sentido agrupar `Amer-Indian-Eskimo` y `Other` en el grupo `Others`, las variables ya que son pocas y concisas, esto lo haremos directamente en la receta.


### Variable Sexo

Ahora procederemos a ver cómo se distribuye la variable **Sexo**:

```{r}
adultos |>
  count(sex, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(sex) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso no hace sentido agrupar ya que son dos categorías muy presentes.

### Variable País

Ahora procederemos a ver cómo se distribuye la variable **País**:

```{r}
adultos |>
  count(native_country, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(native_country) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso solo hace sentido dejar `United States` en un grupo, `Mexico` en otro grupo, `Philippines`,`Germany`,`Canada`,`India`,`England`,`South`,`Italy`,`Japan` y `Taiwan` en el grupo `High Income`, y el resto en el grupo `Others`.

```{r}

adultos <- adultos |> 
 mutate(
    country_transformed = as.factor(case_when(
      native_country %in% c("Philippines","Germany","Canada","India","England","South","Italy","Japan","Taiwan")  ~ "High-Income",
      native_country == "United-States"   ~ as.character(native_country),
      native_country == "Mexico"   ~ as.character(native_country),
      TRUE ~ as.character("Others")
    )))

knitr::kable(adultos |> select(native_country, country_transformed) |>  unique())
```


## Dependencia entre  cuali

Podemos ejecutar un **contraste de independencia** (prueba $\chi^2$ de independencia) para tener mayor certeza de si la predictora es dependiente o no de la variable objetivo (si fuera independiente, no tendría sentido mantenerla)

Podemos hacerlo con **todas las variables a la vez** enfrentándola a la objetivo, entendiendo que cada columna juega el rol de una lista si usamos las funciones del paquete `{purrr}`

```{r warning = FALSE}
chisq <-
  tibble("variable" = adultos |> select(where(is.factor)) |> names(),
         "p_value" = adultos |> select(where(is.factor)) |>
           map_dbl(.f = function(x) { chisq.test(adultos$over_50k, x)$p.value}))
chisq |> arrange(desc(p_value))
```


```{r warning = FALSE}
chisq |> filter(p_value > 0.05)
```

**No hay evidencia suficiente para decir que existe predictora independiente de la objetivo** (al 95% de confianza) según la prueba de independencia realizada


## Variables numéricas

Para las numéricas el proceso será ligeramente diferente, ya que ya no toman modalidades, aunque la mayoría de ellas como veremos podrían funcionar tanto de cuanti como de cuali (recategorizadas)

### Horas de Trabajo a la Semana

* `hours_per_week`: en realidad es una variable cualitativa más que cuantitativa, y a partir de 2 noches en festivo representa menos de 1% --> podríamos probar a **dejarla tal cual o recategorizarla en 4 categorías** (ninguna - 1 - 2 - más de 2)

```{r}
adultos |>
  count(hours_per_week, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc))
```

```{r}

adultos <- adultos |> 
 mutate(
    hours_per_week_transformed = as.factor(case_when(
      hours_per_week<40 ~ as.character("Menos de 40 Horas"),
      hours_per_week>40 ~ as.character("Más de 40 horas"),
      TRUE ~ as.character("40 Horas")
    )))

```

Ahora le daremos un orden a las nuevas categorías:

```{r}
adultos <-
  adultos |>
  mutate(hours_per_week_transformed = factor(hours_per_week_transformed, levels = c("Menos de 40 Horas", "40 Horas", "Más de 40 horas"),
                       ordered = TRUE))
knitr::kable(adultos |> select(hours_per_week_transformed) |> arrange(desc(hours_per_week_transformed)) |> unique() )
```

Y así se vería:

```{r}
adultos|> group_by(hours_per_week_transformed) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 
```

## Colinealidad

Por último, nos falta comprobar los **problemas de  colinealidad** entre las predictoras numéricas. Podemos tratar las **numéricas por separado** (aunque tengamos muchas que en realidad hacen más de cuali que de cuanti)


```{r}
library(corrr)
cor_matrix <- adultos |> select(where(is.numeric)) |> cor() |> round(2)

library(corrplot)
cor_matrix |>
  corrplot( method = 'ellipse', order = 'AOE', type = 'upper')
```


No parece existir una correlación elevada entre ninguna.


## Fase 3: modificación (fuera de la receta)

### Sampling

Con lo observado en la fase de exploración procederemos a hacer un **muestreo estratificado**  del 15% ya que tenemos muchas filas (al menos para hacer pruebas).


```{r}
# Muestreo del 25%
adultos_sample <-
  adultos |>
  group_by(over_50k) |> 
  slice_sample(prop = 0.15) |>
  ungroup()
```


## Fase 3: modificación (dentro de la receta)

### Partición

#### Test vs lo demás

Antes de nada deberemos realizar nuestras particiones, y primero dividiremos en **test y lo demás**, con `initial_split()`, teniendo 25% en test y 75% en todo lo demás

```{r}
# Partición 10% de test 
adultos_split <- initial_split(adultos_sample, strata = over_50k, prop = 0.75)
adultos_split
```

Fíjate que en `adultos_split` solo tenemos las instrucciones. Vamos a aplicarlas

```{r}
# Aplicamos partición
train_data <- training(adultos_split)
test_data <- testing(adultos_split)
```

Tras ello nunca está de más comprobar que efectivamente lo ha realizado de manera estratificada

```{r}
# Comprobamos estratos
train_data |> count(over_50k) |> mutate(porc = 100 * n / sum(n))
test_data |> count(over_50k) |> mutate(porc = 100 * n / sum(n))
```


#### Validación

Dado que no siempre disponemos de un volumen suficiente de datos, una **opción muy común y que arroja unos resultados bastante buenos** es la llamada **validación cruzada v-folds**: dividimos en $v$ trozos nuestro conjunto de entrenamiento, de forma que realizamos las siguientes iteraciones:

* Iteración 1: entrenamos el modelo con los conjuntos $\left\lbrace 2, 3, \ldots, v \right\rbrace$ y validamos con el primer conjunto.

* Iteración 2: entrenamos el modelo con los conjuntos $\left\lbrace 1, 3, \ldots, v \right\rbrace$ y validamos con el segundo conjunto.

...

* Iteración v: entrenamos el modelo con los conjuntos $\left\lbrace 1, 2, \ldots, v1 \right\rbrace$ y validamos con el conjunto $v$-ésimo.

Este método nos permite, no solo no tener que disponer de un alto volumen de datos sino que además **la validación ya no es sobre una sola muestra sino un promedio de varias** (eso sí, muestras relacionadas entre sí, no son independientes). Además con `strata` le diremos que dichas particiones las hagan estratificadas para conservar 0's-1's.


```{r}
# Fijamos la semilla
set.seed(100)

# Declaramos el número de particiones en las que procederemos a validar.
cv_folds <-
 vfold_cv(train_data, 
          v = 5, 
          strata = over_50k,
          repeats = 1) 
```

### Roles

Tras las particiones, el primer paso es **definir la receta**, indicándole el conjunto donde tenemos validación y train, y enfrentar `over_50k` con todas. Después lo que haremos será **asignar posibles roles** que nos puedan diferencias las acciones entre las variables


```{r}
# Receta
rec_adultos <-
  # Fórmula y datos
  recipe(data = train_data, over_50k ~ .)|>
  # Roles
  add_role(where(is.factor), new_role = "cuali") |> 
  add_role(where(is.numeric), new_role = "cuanti") |> 
  add_role(c(workclass, marital_status, occupation, native_country, hours_per_week, education, education_num), new_role = "Eliminar_KNN") |> 
  add_role(c(workclass, marital_status, occupation, native_country, hours_per_week, education, education_num), new_role = "Eliminar_Arboles") |> 
  add_role(c(workclass, marital_status, occupation, native_country, hours_per_week, education, education_num), new_role = "Eliminar_Random_Forests")
```


###Receta KNN

Ahora procederemos en crear la receta par el algoritmo KNN

```{r}
# Receta
KNN_rec_adultos <-
  rec_adultos  |> 
  # Modificación de variables actuales
  step_mutate(
    relationship =
                as.factor(case_when(
      relationship %in% c("Wife","Husband")  ~ "Boss",
      TRUE ~ "Others")),
    race =
                as.factor(case_when(
      race %in% c("Amer-Indian-Eskimo","Other")  ~ "Others",
      TRUE ~ as.character(race)))) |>
  # Eliminamos variables predefinidas.
  step_rm(has_role("Eliminar_KNN"))|> 
  # Detectar outliers
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "iqr")) > 1.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes (podríamos dejarlas como una categoría más)
  step_impute_knn(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Remover nulos
  step_naomit(everything(), skip = TRUE) |> 
  # Agrupamos en Minority los valores menos a 5%
  step_other(has_role("cuali"), threshold = .05, other = "Minority")  |> 
  # Escalamos los valores
  step_range(has_role("cuanti")) |> 
  # Normalizamos
  step_normalize(has_role("cuanti")) |> 
  # Dummyficamos
  step_dummy(all_nominal(), -all_outcomes())  |> 
  step_zv(all_predictors()) |> 
  step_corr(has_role("cuanti"), threshold = 0.7)  #|> 
```

Probaremos la receta para ver sus resultados:

```{r}
prepped_data <- 
  KNN_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```

### Receta CART

Ahora procederemos con la receta para crear el modelo de árboles de decisión.

```{r}
# Receta
cart_rec_adultos <-
  rec_adultos  |> 
  # Removeremos variables
  step_rm(has_role("Eliminar_Arboles"))|> 
  # Detectaremos outliers con base al "z"test.
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "z")) >2.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes
  step_impute_median(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Removemos valores nulos
  step_naomit(everything(), skip = TRUE) |> 
  # Agrupamos en Minority los valores menos a 5%
  step_other(has_role("cuali"), threshold = .05, other = "Minority")  |> 
  # Eliminamos variables con varianza 0
  step_zv(all_predictors()) |> 
  # Eliminamos variables con correlación mayor al 70%
  step_corr(has_role("cuanti"), threshold = 0.7)  |> 
  # Sobremuestreo 66% - 33%
  themis::step_upsample(over_50k, over_ratio = 0.5) 
```

Probamos la receta:

```{r}
cart_prepped_data <- 
  cart_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(cart_prepped_data)
```

### Receta Random Forest

Ahora procederemos a crear la receta para la modelación con el algoritmo Random Forest:

```{r}
# Receta
rf_rec_adultos <-
  rec_adultos  |> 
  # Agru
  step_rm(has_role("Eliminar_Random_Forests"))|> 
  # Detectar outliers
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "z")) >2.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes (podríamos dejarlas como una categoría más)
  step_impute_knn(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Removemos nulos
  step_naomit(everything(), skip = TRUE) |>
  # Agrupamos con Tag Minority a los grupos de valores en las variables que representen menos del 4%
  step_other(has_role("cuali"), threshold = .04, other = "Minority")  |> 
  # Reagrupamos
  step_zv(all_predictors()) |> 
  step_corr(has_role("cuanti"), threshold = 0.7)  |> 
  # Sobremuestreo 50-50%
  themis::step_upsample(over_50k, over_ratio = 1) 
```


Probamos la receta para el algoritmo de Random Forest:

```{r}
rf_prepped_data <- 
  rf_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(rf_prepped_data)
```

# Fase 4 Modelling

## Metodología

Primero evaluaremos cada uno de los modelos por separado (KNN, CART, Random Forest) utilizando slice_sample seleccionaremos aleatoriamente 10 diferentes combinaciones de parámetros y con procesamiento en paralelo seleccionaremos el que tenga mejores resultados de cada uno de ellos. Al terminar la selección del mejor modelo, los compararemos entre ellos y seleccionaremos al mejor para hacer un Grid Search completo con validación cruzada y doble repetición para encontrar el mejor resultado posible.


## Cómputo en Paralelo

Vamos a **paralelizar en NUESTRO PROPIO ORDENADOR**: un ordenador suele tener **varios procesadores o cores** que pueden funcionar de manera «independiente» uno de otro. Vamos a detectar la cantidad de núcleos de los que podemos disponer con `detectCores()`.

```{r}


library(parallel)
library(doParallel)


# Detectamos los cores que tenemos
detectCores()
```

A la hora de paralelizar es importante que lo hagamos con cuidado ya que puede que nuestro ordenador se quede colgado: mi consejo es que definas el número de cores a usar como los que tienes menos uno.

Con `makeCluster()` montamos los **clúster en cada nodo** y con `registerDoParallel()` registramos la paralelización (puedes ver los hilos abiertos con `showConnections()`).

```{r}
# Iniciamos la paralelización
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()
```

## Fase 4 KNN: modelo y flujo

### Flujo Modelo KNN

Empezamos a realizar el modelo KNN

```{r}
# Modelo
knn_model <-
  nearest_neighbor(mode = "classification", neighbors = tune("k"),
                   weight_func = tune("weight"), dist_power = tune("dist")) |>
  set_engine("kknn")

# Flujo de trabajo
knn_wflow_adultos <-
  workflow() |>
  add_recipe(KNN_rec_adultos) |>
  add_model(knn_model)
```

### Grid KNN

Ahora formaremos el grid para el modelo KNN

```{r}
grid_knn <-
  extract_parameter_set_dials(knn_wflow_adultos) |>
  # Actualizamos
  update(k = neighbors(range = c(90, 110)),
         weight = weight_func(values = c("inv","gaussian")),
         dist = dist_power(range = c(4, 6))) |>
  grid_regular(levels = 3) 
grid_knn <- grid_knn |> slice_sample(n=10) # 18 modelos (3 x 2 x 3)
grid_knn
```
### Aplicación del flujo de trabajo

```{r}
# Aplicamos el flujo
knn_res <- 
  knn_wflow_adultos |> 
  tune_grid(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas),
    grid = grid_knn,
    control = control_grid(verbose = TRUE, allow_par = TRUE, #<<
                                   pkgs = c("outliers")),
 ) 

# finalizamos clusters
stopCluster(make_cluster)
registerDoSEQ()

# Recolectamos las métricas de error
knn_res |> collect_metrics(summarize = TRUE)

```

### Matriz de Confusión

Ahora es importante saber cómo se visualiza la matriz de confusión

```{r}

knn_pred <- 
  knn_res |>
  collect_predictions() 


knn_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```

### Curva ROC 

Hacemos la curva ROC para medir la calidad de la predicción

```{r}

knn_pred |> 
  group_by(id) |>
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot() +
  theme_economist_white()
```


# Densidad de Predicción

Veamos la densidad de nuestra predicción.

```{r}

knn_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```

Podemos ver cómo nuestro modelo se comporta totalmente diferente al poder definir las personas con un ingreso mayor a 50 K las cuales se encuentran en su mayoría a partir del 77% de probabilidad de pertenencia.

## Fase 4 CART: modelo y flujo

### Flujo Modelo CART

Empezamos a realizar el modelo CART

```{r}
# Modelo
cart_model <- decision_tree(mode = "classification", tree_depth = 7,
                min_n = 0.025 * nrow(test_data),
                cost_complexity = 0.1) |> set_engine("rpart")

# Flujo de trabajo
cart_wflow_adultos <-
  workflow() |>
  add_recipe(cart_rec_adultos) |>
  add_model(cart_model)

#Ejecución del flujo de trabajo
cart_res <- 
  cart_wflow_adultos |> 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens),
    control = control_resamples(save_pred = TRUE)
    )

# Colección de métricas
cart_res |> collect_metrics(summarize = TRUE)
```



```{r}

cart_pred <- 
  cart_res |>
  collect_predictions()


cart_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```


```{r}
# Matriz de confusión: etiqueta real vs etiqueta predicha
cart_pred |> 
  group_by(id) |> # id contains our folds
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot()

```



```{r}
cart_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```


### Visualizar el árbol

También podemos visualizar fácilmente los árboles generados con CART (Gini, del paquete `{rpart}`): primero con `extract_fit_engine()` extraemos las rutas del árbol, y después con `rpart.plot(roundint = FALSE, extra = 4)` le indicamos que no redondee valores a enteros y de los modelos de visualización elegimos `extra = 4` (prueba con varios para ver las diferencias). Para visualizar árboles `C5.0` ver documentación en <https://topepo.github.io/C5.0/reference/plot.C5.0.html>

```{r visualizar}
library("rpart.plot")
# Aplicamo flujo

# Calculamos probabilidades

cart_best_model <- 
  cart_res |> select_best("roc_auc")

final_cart <- 
  cart_wflow_adultos |> 
  finalize_workflow(cart_best_model)

adultos_knn_fit <- 
  final_cart |>
  last_fit(adultos_split) 

# Calculamos métricas en test (las indicadas)
adultos_knn_fit |> collect_metrics()

adultos_knn_fit |>
 extract_fit_engine() |>
  rpart.plot(roundint = FALSE, extra = 4)
```

### Importancia de las variables

```{r}
library(vip)

last_fit_rf %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```
## Fase 4 Random Forest: modelo y flujo


```{r}
library(ranger)


# Modelo
rf_spec <- 
  rand_forest() |> 
  set_engine("ranger", importance = "impurity") |> 
  set_mode("classification")


# Flujo de trabajo
rf_wflow <-
 workflow() |>
 add_recipe(rf_rec_adultos) |> 
 add_model(rf_spec) 

rf_res <- 
  rf_wflow |> 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens),
    control = control_resamples(save_pred = TRUE)
    ) 

rf_res |> collect_metrics(summarize = TRUE)
```



```{r}

rf_pred <- 
  rf_res |>
  collect_predictions()


knn_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```

```{r}
# Matriz de confusión: etiqueta real vs etiqueta predicha
rf_pred |> 
  group_by(id) |> # id contains our folds
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot()

```



```{r}
rf_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```


## Comparación de Modelos

```{r}


rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")

cart_metrics <- 
  cart_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "CART")

knn_metrics <- 
  knn_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "KNN")

# creamos un dataframe con todos los modelos
model_compare <- bind_rows(rf_metrics,
                           cart_metrics,
                           knn_metrics,
                           ) 

# Cambiando la estructura de datos
model_comp <- 
  model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# 
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), y = mean_roc_auc + 0.08),
     vjust = 1
  )
```

```{r}
model_comp %>% slice_max(mean_roc_auc)
```
 
 
### Comparación Curva ROC
  
## Fase 4 Hyper Tunning 


Con base a los resultados podemos concluir que el modelo más adecuado es el **Random Forest** con los siguientes parámetros:



